# -*- coding: utf-8 -*-
"""Server.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TRR60PNhgonEfPLGtnzNEgpBIS_hj4Tc
"""

# 1. Install Dependencies
!pip install fastapi uvicorn python-multipart pyngrok nest-asyncio
!pip install torch transformers peft accelerate bitsandbytes qwen-vl-utils

# 2. Authenticate Ngrok (Replace with YOUR actual token)
from pyngrok import ngrok
NGROK_TOKEN = "37RaUjjBTmUdoZURfjJG9lfAnfG_SFCcz3KN3ZyGXq8f3Y1j"  # <--- PASTE HERE
ngrok.set_auth_token(NGROK_TOKEN)

#========================================
# 2. MOUNT GOOGLE DRIVE
# ==========================================
from google.colab import drive
import os

if not os.path.exists('/content/drive'):
    print("Mounting Google Drive...")
    drive.mount('/content/drive')

# UPDATE THIS if your folder name is different in Drive
# This assumes 'qwen_price_finetuned' is right inside your 'My Drive'
ADAPTER_PATH_DRIVE = "/content/drive/MyDrive/qwen_price_finetuned"

# Verify model exists
if not os.path.exists(ADAPTER_PATH_DRIVE):
    raise FileNotFoundError(f"Could not find model at {ADAPTER_PATH_DRIVE}. Please check your Drive folder name.")

# ==========================================
# 3. SETUP NGROK TUNNEL
# ==========================================
from pyngrok import ngrok

# --- PASTE YOUR TOKEN BELOW ---
NGROK_AUTH_TOKEN = "37RaUjjBTmUdoZURfjJG9lfAnfG_SFCcz3KN3ZyGXq8f3Y1j"
# ------------------------------

ngrok.set_auth_token(NGROK_AUTH_TOKEN)

# ==========================================
# 4. CREATE SERVER FILE (server.py)
# ==========================================
server_code = f"""
import os
import re
import torch
import uvicorn
from contextlib import asynccontextmanager
from fastapi import FastAPI, UploadFile, File, Form
from fastapi.responses import JSONResponse
from PIL import Image
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig
from peft import PeftModel
from qwen_vl_utils import process_vision_info
import io

# PATHS
ADAPTER_PATH = "{ADAPTER_PATH_DRIVE}"
BASE_MODEL_ID = "Qwen/Qwen2.5-VL-3B-Instruct"

ml_models = {{}}

@asynccontextmanager
async def lifespan(app: FastAPI):
    print("--- SERVER STARTUP: Loading Model... ---")

    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16
    )

    base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        BASE_MODEL_ID,
        device_map="auto",
        quantization_config=bnb_config,
        trust_remote_code=True,
    )

    print(f"Loading Adapter from: {{ADAPTER_PATH}}")
    model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)
    model.eval()

    processor = AutoProcessor.from_pretrained(ADAPTER_PATH, trust_remote_code=True)

    ml_models["model"] = model
    ml_models["processor"] = processor

    print("--- MODEL LOADED SUCCESSFULLY ---")
    yield
    ml_models.clear()
    torch.cuda.empty_cache()

app = FastAPI(title="Qwen Price Predictor API", lifespan=lifespan)

from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allows all origins
    allow_credentials=True,
    allow_methods=["*"],  # Allows all methods
    allow_headers=["*"],  # Allows all headers
)

@app.get("/")
def home():
    return {{"message": "Price Prediction Server is Running. Go to /docs to test."}}

@app.post("/predict")
async def predict_price(
    product_name: str = Form(...),
    category: str = Form(...),       # NEW INPUT
    condition: str = Form(...),      # NEW INPUT
    age: str = Form(...),            # NEW INPUT
    file: UploadFile = File(...)
):
    model = ml_models["model"]
    processor = ml_models["processor"]

    try:
        image_data = await file.read()
        image = Image.open(io.BytesIO(image_data)).convert("RGB")

        # --- UPDATED PROMPT LOGIC ---
        # We inject the new details into the prompt so the model context includes them.
        prompt_text = (
            f"Analyze this image to estimate the resale price.\\n"
            f"Product Name: {{product_name}}\\n"
            f"Category: {{category}}\\n"
            f"Condition: {{condition}}\\n"
            f"Usage Duration: {{age}} years\\n"
            f"Task: Based on the visual evidence and these details, predict the final_price in INR. Output only the numeric value."
        )

        messages = [
            {{
                "role": "user",
                "content": [
                    {{"type": "image", "image": image}},
                    {{"type": "text", "text": prompt_text}}
                ]
            }}
        ]

        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        image_inputs, video_inputs = process_vision_info(messages)

        inputs = processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt",
        ).to("cuda")

        with torch.no_grad():
            generated_ids = model.generate(**inputs, max_new_tokens=16)
            output_text = processor.batch_decode(
                generated_ids[:, inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )[0]

        # Regex to extract the first number found in the response
        match = re.search(r"(\d[\d,]*(\.\d+)?)", output_text)
        if match:
            price_float = float(match.group(1).replace(',', ''))
            return {{
                "product": product_name,
                "predicted_price": price_float,
                "currency": "INR",
                "details": {{
                    "category": category,
                    "condition": condition,
                    "age": age
                }},
                "raw_output": output_text
            }}
        else:
            return JSONResponse(
                status_code=422,
                content={{"error": "Could not parse number", "raw_output": output_text}}
            )

    except Exception as e:
        import traceback
        traceback.print_exc()
        print(str(e))
        return JSONResponse(status_code=500, content={{"error": str(e)}})

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
"""

with open("server.py", "w") as f:
    f.write(server_code)

# ==========================================
# 5. START SERVER
# ==========================================
# Open tunnel to port 8000
public_url = ngrok.connect(8000)
print(f"\n\nðŸš€ PUBLIC API URL: {public_url}\n")
print("Click the link above, then add '/docs' to the end (e.g., https://...ngrok-free.app/docs)")

# Run the server
!python server.py

